# EKS

Date: 02/05/2021

## Status

✅ Accepted

## What’s proposed

Use Amazon EKS for running the main cluster, which hosts MOJ service teams' applications. This replaces usage of kOps.

## Reasoning

Benefits of EKS:

* a managed control plane, reducing operational overhead compared to kOps, such as scaling the control plane nodes
* managed nodes, further reducing operational overhead

We've gained a lot experience of using EKS with the Manager cluster

## EKS configuration decisions

### Auth

**Chosen:**: OIDC, with Auth0 as broker and GitHub as Identity Provider

Developers in service teams need to use the k8s auth, and GitHub continues to be the most common SSO amongst them with good tie-in to JML processes - see [ADR 6 Use GitHub as our identity provider](006-Use-github-as-user-directory.md)

Auth0 is useful as a broker, because it adds the GitHub team, used by k8s RBAC authorization. (check)

Future options:

* Azure AD SSO is growing in MOJ - there's a case for switching to that, if it is adopted amongst our users
* IAM auth has the benefit of immediately revoking access. Maybe we could use federated login with GitHub? (But would that give only temporary kubecfg?) Or sync the GitHub team info to IAM?

### Auth credential issuer

**Chosen:**: Cloud Platform Kuberos

We've long used Kuberos for issuing kubecfg credentials to users. The [original version of Kuberos](https://github.com/negz/kuberos) is unmaintained, but it's pretty simple, so we have updated the dependencies in: https://github.com/ministryofjustice/cloud-platform-kuberos/ and will maintain our fork.

Other options considered:

* [Gangway](https://github.com/heptiolabs/gangway) - similar to Kuberos, it has not had releases for 2 years (v3.2.0)
* [kubelogin](https://github.com/int128/kubelogin)
  * CP team would have to distribute the client secret to all users. It seems odd to go to the trouble of securely sharing that secret, to overcome the perceived difficulty of issuing kubecfg credentials.
  * Requires all of our users to install the software, rather than doing it server-side centrally
* [kubehook](https://github.com/negz/kubehook) - not compatible with EKS - doesn't support web hook authn
* [dex](https://github.com/dexidp/dex) - doesn't have a web front-end for issuing creds - it is more of an OIDC broker

### Authorization

**Chosen:**: existing RBAC configuration

We'll continue to use our existing RBAC configuration from the previous cluster.

### Node management

**Chosen:**: Managed node groups, with future experimenting with Fargate

Options:

* Self-managed nodes
* Managed node groups - automates various aspects of the node lifecycle, including creating the EC2s, the auto scaling group, registration of nodes with kubernetes and recycling nodes
* Fargate nodes - fully automated nodes, the least to manage. Benefits from more isolation between pods and automatic scaling. Doesn't support daemonsets.

We aim to take advantage of as much automation as possible, to minimize the team's operational overhead and risk. Initially we'll use managed node groups, before looking at Fargate for workloads.

#### Future Fargate considerations

*Pod limits* - there is a quota limit of [500 Fargate pods per region per AWS Account](https://aws.amazon.com/about-aws/whats-new/2020/09/aws-fargate-increases-default-resource-count-service-quotas/) which could be an issue, considering we currently run ~2000 pods. We can request AWS raise the limit - not currently sure what scope there is. With Multi-cluster stage 5, the separation of loads into different AWS accounts will settle this issue.

*Daemonset functionality* - needs replacement:

* fluent-bit - currently used for log shipping to ElasticSearch. AWS provides a managed version of [Fluent Bit on Fargate](https://aws.amazon.com/blogs/containers/fluent-bit-for-amazon-eks-on-aws-fargate-is-here/) which can be configured to ship logs to ElasticSearch.
* prometheus-node-exporter - currently used to export node metrics to prometheus. In Fargate the node itself is managed by AWS and therefore hidden. However we can [collect some useful metrics about pods running in Fargate from scraping cAdvisor](https://aws.amazon.com/blogs/containers/monitoring-amazon-eks-on-aws-fargate-using-prometheus-and-grafana/), including on CPU, memory, disk and network

*No EBS support* - Prometheus will run still in a managed node group. Likely other workloads too to consider.

*how people check the status of their deployments* - to be investigated

*ingress can't be nginx? - just the load balancer in front* - to be investigated

If we don't use Fargate then we should take advantage of Spot instances for reduced costs. However Fargate is the priority, because the main driver here is engineer time, not EC2 cost.

### Node groups

**Chosen:** One node group initially, with a couple of instance types

Start with one node group, to minimize the operation overhead.

In the node group we will specify 2 or 3 related instance types, because with only 1 there is a risk that the cloud provider runs out of that type within the region.

Further node groups may be valueable for using Spot or Fargate nodes, as we explore them in the future.

### Node instance types

**Chosen:** t3a.xlarge, t3.xlarge, t3a.large - for the main node group

Due to the choice of AWS CNI networking in the new cluster, we have a new limit on pods per node, due to limitations on ENIs / IP addresses. With our current instance type `r5.xlarge` we would be limited to 58 pods per node, costing $32/pod/year. In comparison, on our existing kops cluster we see the average pod actually using or reserving 300MB memory, so if we filled a worker node 90% (up to the alert/autoscaling threshold), we'd fit 94 pods in, at a cost of $20/pod/year. Analysis shows that the closest to that we can get, taking into account the IP limitation with the AWS CNI, is with t3a.xlarge, which has a better ratio of IP addresses to memory than the r5/r4 ranges, and lower price per GB than the c5 range. Filling a t3a.xlarge instance with our 300MB pods, we hit the memory limit before the IP limit, fitting in 47 pods, and the cost is $23/pod/year - not far off the existing $20/pod/year. In terms of the cluster size, in selecting the t3a.xlarge nodes, and maintaining 30% extra capacity beyond the average of 2000 pods (because the workload fluctuates), we'd move from 27 worker nodes to 56, and the annual cost of the instances increases from $52k to $60k (not yet accounting for discounts).

So the primary choice is:

* t3a.xlarge - 4 vCPUs, 16GB memory, 58 IPs per node, $23/pod/year

With fallbacks (should the cloud provider run out of these in the AZ):

* t3.xlarge - 4 vCPUs, 16GB memory, 58 IPs per node, $26/pod/year
* t3a.large - 2 vCPUs, 8GB memory, 58 IPs per node, $24/pod/year - would need to increase number of nodes though

In the future we might consider the ARM processor ranges, but we'd need to consider the added complexity of cross-compiled container images.

Workings:

* [Spreadsheet of instance types](https://docs.google.com/spreadsheets/d/1JpZudxmGkP-JNpOs36hipFPbWGCCkaCmi9pP_Rc9yng/edit)
* [Pod memory needs - workings](https://docs.google.com/document/d/1zw3gFKSid9768dZREO5XNUS7jBsPDd4ctIezfSpyBbo/edit)

**Chosen:** r5.2xlarge, r4.2xlarge - for the Prometheus node group

The existing cluster uses r5.2xlarge, so we'll continue with that, and add some fall-backs:

* r5.2xlarge - memory optimized range -	8 cores 64 GB
* r4.2xlarge - memory optimized range -	8 cores 61 GB

### Pod networking (CNI)

**Chosen:** AWS VPC networking (CNI)

Link: https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html

AWS's CNI is used for the pod networking (IPAM, CNI and Routing). Each pod is given an ENI and IP address from the underlying VPC (rather than an overlay network). Pod traffic is routed between nodes using the native VPC.

Advantages of AWS's CNI:

* it is the default with EKS, native to AWS, is fully supported by AWS - low management overhead
* offers good network performance

The concern with AWS's CNI would be that it uses an IP address for every pod, and there is a [limit per node](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt), depending on the EC2 instance type and the number of ENIs it supports. The calculations in [Node Instance Types](#node-instance-types) show that with a change of instance type, the cost of the cluster increases by 17% or $8k, which is acceptable - likely less than the engineering cost of maintaining and supporting full Calico networking and custom node image.

The alternative considered was [Calico networking](https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-calico-networking). This has the advantage of not needing an IP address per pod, and associated instance limit. And it is open source. However:

* We wouldn't have any support from the cloud provider if there were networking issues.
* We have to maintain a customized image with Calico installed. It's likely that changes to EKS over time will frequently cause breakages with this networking setup.
* Installation requires recycling the nodes, which is not a good fit with declarative config.

### Node image

**Chosen:** Amazon EKS optimized AMIs

Link: https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html

These EKS AMIs are the default and therefore lowest maintenance. We can't see a reason not to use them. (If we needed changes, then a launch config would give us flexibility.)

### Cluster scaling

**Chosen:** Manual cluster scaling, with future consideration of auto-scaling

Initially we will continue manually scaling up/down the nodes manually, as we did on the kops cluster. This has proven fine, since the workload changes only slowly. And we have alerts set-up to warn when capacity is low, to prompt the team to scale the cluster.

Cluster auto-scaling should be considered soon though. This is to embrace one of the cloud's key advantages - to scale up/down swiftly, for good efficiency. For example, usage of the hosted sites is much lower at night, so we should aim to get our workloads to scale down automatically, and our cluster to follow suit. In addition, tenants' non-production environments do not need to run at all at night.

Considerations for auto-scaler:

* we need to maintain spare capacity, so that workloads that scale up don't have to wait for nodes to start-up, which can take about 7 minutes. This may require some tuning.
* tenants should be encouraged to auto-scale their pods effectively (e.g. using the Horizontal pod autoscaler), to capitalize on cluster auto-scaling
* scaling down non-prod namespaces will need agreement from service teams

### Network policy enforcement

**Chosen:** Calico network policy enforcement (i.e. just for policy, not the full blown Calico networking)

Link: https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-amazon-vpc-networking

This implements the Kubernetes Network Policy API, needed to isolate tenants. It is [recommended by EKS](https://docs.aws.amazon.com/eks/latest/userguide/calico.html). The CP team used it in the previous live-1 cluster, and are familiar with it.

CP's namespaces have a [default NetworkPolicy](https://github.com/ministryofjustice/cloud-platform-environments/blob/main/namespace-resources-cli-template/04-networkpolicy.yaml) that only allows incoming requests from pods in the same namespace and ingress-controller namespace, not other tenants' namespaces.

An alternative is [AWS Security Groups](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html). Creating a 'SecurityGroupPolicy' resource for a namespace would attach a security group, allowing us to specify the inbound and outbound network traffic. The [advantage](https://github.com/aws/containers-roadmap/issues/177) over a similar NetworkPolicy, is that the Security Group can specify AWS resources like RDS instances. So it would be a network restriction, in addition to the auth restriction (CP's RDS instances are currently [network accessible across the VPC](https://user-guide.cloud-platform.service.justice.gov.uk/documentation/other-topics/rds-external-access.html#accessing-your-rds-database)). We should explore this in the future for defence in depth.

**Chosen:** Calico Typha

Install Typha for performance benefits. It caches requests made by Calico Felix to the k8s API.

### Control plane logging

**Chosen:** enable all cluster logging

Link: https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html

Control plane logs can be enabled to log: k8s API, authentication, controller manager and scheduler

The logs go to CloudWatch. Maybe we need to export them elsewhere. Further discussion is covered in: [ADR 23 Logging](023-Logging.md)

### Node terminal access

**Chosen:** AWS Systems Manager, installed as daemonset; no bastion nodes

Links:

* https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/install-ssm-agent-on-amazon-eks-worker-nodes-by-using-kubernetes-daemonset.html
* https://github.com/aws/containers-roadmap/issues/593

AWS Systems Manager benefits:

* easy to install - daemonset
* auth is via a team member's AWS creds, so it's tied into JML processes and access can be removed immediately if they leave the team
* terminal commands are logged - useful for audit purposes
* [it's an EKS best practice](https://aws.github.io/aws-eks-best-practices/security/docs/hosts/#minimize-access-to-worker-nodes)

To note:

* requires permissions `hostNetwork: true` and `privileged: true` so may need its own PSP
* it's no use if the node is failing to boot or join the cluster properly, but we can live with that - it's likely that it's the pods we want to characterize, not the node, because the node is managed

The traditional method of node access would be to SSH in via a bastion. This involves a shared ssh key, and shared credentials is not an acceptable security practice.

Implementation ticket: https://github.com/ministryofjustice/cloud-platform/issues/2962

### PodSecurityPolicies

**Chosen:** Default EKS PSP `eks.privileged` should be used only by `aws-node` pod

**Chosen:** Apply CP's existing `privileged` PSP only to the `kube-system` namespace and `restricted` PSP to the rest

EKS comes with PSP `eks.privileged` that allows anything. However, security best practice is to limit it, to ensure most pods aren't allowed to run as privileged or root. [EKS best practices says]: "we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace". In practice we found only `aws-node` pod needed it.

CP already has `privileged` and `restricted` PSPs which we can apply as on the existing cluster.

#### PSP Deprecation

PSPs are [deprecated](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/) as of k8s 1.21 (April 2021) and will be removed 1.25 (due April 2022). This is because PSPs are considered a difficult way to specify policy. It's not clear what will replace them, but OPA Gatekeeper is a possibility. We will look to move from PSPs to whatever standard is selected in the KEP over coming months.

### Apps' access to AWS roles

**Chosen:** IAM Roles for Service Accounts (IRSA), instead of kiam

**Chosen:** Block access to instance metadata's node role

#### IRSA

Benefits of IRSA over kiam or kube2iam:

* kiam/kube2iam require running and managing a daemonset container.
* kiam/kube2iam require [powerful AWS credentials](https://github.com/jtblin/kube2iam#iam-roles), which allow EC2 boxes to assume any role. Appropriate configuration of kiam/kube2iam aims to provide containers with only a specific role. However there are security concerns with this approach:
  * With kube2iam you have to remember to set a `--default-role` to use when annotation is not set on a pod.
  * When a node boots, there may be a short window until kiam/kube2iam starts up, when there is no protection of the instance metadata. In comparison, IRSA injects the token into the pod, avoiding this concern.
  * With kube2iam/kiam, an attacker able to get root on the node could access the credentials and therefore any AWS Role. In comparison, with IRSA a breach of k8s might only have bring access to the AWS Roles that are associated with k8s service roles.

#### Blocking access instance metadata

The EKS node needs various AWS permissions, and these are provided by the node role. To avoid pods using these persmissions, we should block access. Implementation details are provided here: https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node
