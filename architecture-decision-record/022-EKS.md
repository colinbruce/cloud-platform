# EKS

Date: 27/05/2021

## Status

ðŸ¤” Proposed

## Whatâ€™s proposed

Use Amazon EKS for running the main cluster, which hosts MOJ service teams' applications. This replaces usage of kOps.

## Reasoning

Benefits of EKS:

* a managed control plane, reducing operational overhead compared to kOps, such as scaling the control plane nodes
* managed nodes, further reducing operational overhead

We've gained a lot experience of using EKS with the Manager cluster

## EKS configuration decisions

### Auth

**Chosen:**: OIDC, with Auth0 as broker and GitHub as Identity Provider

Developers in service teams need to use the k8s auth, and GitHub continues to be the most common SSO amongst them with good tie-in to JML processes - see [6. Use GitHub as our identity provider](006-Use-github-as-user-directory.md)

Auth0 is useful as a broker, because it adds the GitHub team, used by k8s RBAC authorization. (check)

Future options:

* Azure AD SSO is growing in MOJ - there's a case for switching to that, if it is adopted amongst our users
* IAM auth has the benefit of immediately revoking access. Maybe we could use federated login with GitHub? (But would that give only temporary kubecfg?) Or sync the GitHub team info to IAM?

### Auth credential issuer

**Chosen:**: Cloud Platform Kuberos

We've long used Kuberos for issuing kubecfg credentials to users. The [original version of Kuberos](https://github.com/negz/kuberos) is unmaintained, but it's pretty simple, so we are updating the dependencies in: https://github.com/ministryofjustice/cloud-platform-kuberos/

Other options considered:

* [kubelogin]()
* [Gangway]()
* [dex]()
* [kubehook](https://github.com/negz/kubehook) - not compatible with EKS - doesn't support web hook authn

### Node management

**Chosen:**: Managed node groups, with future experimenting with Fargate

Options:

* Self-managed nodes
* Managed node groups - automates various aspects of the node lifecycle, including creating the EC2s, the auto scaling group, registration of nodes with kubernetes and recycling nodes
* Fargate nodes - fully automated nodes, the least to manage. Benefits from more isolation between pods and automatic scaling. Doesn't support daemonsets.

We aim to take advantage of as much automation as possible, to minimize the team's operational overhead and risk. Initially we'll use managed node groups, before looking at Fargate for workloads.

#### Future Fargate considerations

*Pod limits* - there is a quota limit of [500 Fargate pods per region per AWS Account](https://aws.amazon.com/about-aws/whats-new/2020/09/aws-fargate-increases-default-resource-count-service-quotas/) which could be an issue, considering we currently run ~2000 pods. We can request AWS raise the limit - not currently sure what scope there is. With Multi-cluster stage 5, the separation of loads into different AWS accounts will settle this issue.

*Daemonset functionality* - needs replacement:

* fluent-bit - currently used for log shipping to ElasticSearch. AWS provides a managed version of [Fluent Bit on Fargate](https://aws.amazon.com/blogs/containers/fluent-bit-for-amazon-eks-on-aws-fargate-is-here/) which can be configured to ship logs to ElasticSearch.
* prometheus-node-exporter - currently used to export node metrics to prometheus. In Fargate the node itself is managed by AWS and therefore hidden. However we can [collect some useful metrics about pods running in Fargate from scraping cAdvisor](https://aws.amazon.com/blogs/containers/monitoring-amazon-eks-on-aws-fargate-using-prometheus-and-grafana/), including on CPU, memory, disk and network

*No EBS support* - Prometheus will run still in a managed node group. Likely other workloads too to consider.

*how people check the status of their deployments* - to be investigated

*ingress can't be nginx? - just the load balancer in front* - to be investigated

### Node instance types

**Chosen:** r5.xlarge, r4.xlarge, c5.xlarge

Existing cluster uses:

* r5.xlarge - memory optimized range - 4 cores 32 GB
* r4.xlarge - memory optimized range - 4 cores 30.5 GB
* c5.xlarge - compute optimized range - 4 cores 8 GB

The emphasis on memory optimized nodes is born of experience of running out of memory more than CPU. These nodes work for the current workload, so we will continue with them for the time being.

### Node image

**Chosen:** Amazon EKS optimized AMIs

Link: https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html

These EKS AMIs are the default and therefore lowest maintenance. We can't see a reason not to use them. (If we needed changes, then a launch config would give us flexibility.)

### Node groups

**Chosen:** One node group initially, with a couple of instance types

Start with one node group, to minimize the operation overhead.

In the node group we will specify 2 or 3 related instance types, because with only 1 there is a risk that the cloud provider runs out of that type within the region.

Further node groups may be valueable for using Spot or Fargate nodes, as we explore them in the future.

### Pod networking (CNI)

**Chosen:** AWS VPC networking (CNI) (with built-in Calico network policy enforcement)

Links:
* https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html
* https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-amazon-vpc-networking

AWS's CNI is the default with EKS. It is fully supported by AWS. It will have a low management overhead.

The concern with this solution would be that it uses an ENI / IP address for every node, and there is a limit, depending on the EC2 instance type. Our current `r5.xlarge` is limited to 58 pods per node. Currently on kops we have `kubectl get nodes | grep node | wc -l` 29 nodes, and the EKS equivalent would be `kubectl get pods -A | wc -l` 1926 pods / 58 = 33 nodes. These extra 4 nodes would cost $0.296*24*30 = $213/month, which is manageable. Other instances with higher limits are also possible (up to the kubelet limit of 110 pods per node). And we have 8192 IPs per subnet - this should be plenty.

The alternative considered was [Calico networking](https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-calico-networking). This has the advantage of not needing an IP address per pod, and associated instance limit. And it is open source. However:

* Installation requires recycling the nodes, which is not a good fit with declarative config.
* We wouldn't have any support from the cloud provider if there were networking issues.
* It's likely that changes to EKS over time will frequently cause breakages with this networking setup.

We have chosen to use the built-in Calico network policy. We used Calico in the previous live-1, and are familiar with it. An alternative is AWS Security Groups, which we could explore in the future.

**Chosen:** Calico Typha

TODO caching

### Control plane logging

**Chosen:** enable all cluster logging

Link: https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html

Control plane logs can be enabled to log: k8s API, authentication, controller manager and scheduler

The logs go to CloudWatch, so we'd use CloudWatch exporter to get the logs into ElasticSearch, since ElasticSearch is our centralized logging. TODO limits with CLoudWatch

### Node terminal access

**Chosen:** AWS Systems Manager, installed as daemonset; no bastion nodes

Links:

* https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/install-ssm-agent-on-amazon-eks-worker-nodes-by-using-kubernetes-daemonset.html
* https://github.com/aws/containers-roadmap/issues/593

AWS Systems Manager benefits:

* easy to install - daemonset
* auth is via a team member's AWS creds, so it's tied into JML processes and access can be removed immediately if they leave the team
* terminal commands are logged - useful for audit purposes
* [it's an EKS best practice](https://aws.github.io/aws-eks-best-practices/security/docs/hosts/#minimize-access-to-worker-nodes)

To note:

* requires permissions `hostNetwork: true` and `privileged: true` so may need its own PSP
* it's no use if the node is failing to boot or join the cluster properly, but we can live with that - it's likely that it's the pods we want to characterize, not the node, because the node is managed

The traditional method of node access would be to SSH in via a bastion. This involves a shared ssh key, and shared credentials is not an acceptable security practice.

### PodSecurityPolicies

**Chosen:** Default EKS PSP `eks.privileged` should be used only by `aws-node` podnamespace

**Chosen:** Apply CP's existing `privileged` PSP only to the `kube-system` namespace and `restricted` PSP to the rest

EKS comes with PSP `eks.privileged` that allows anything. However, security best practice is to limit it, to ensure most pods aren't allowed to run as privileged or root. [EKS best practices says]: "we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace". In practice we found only `aws-node` pod needed it.

CP already has `privileged` and `restricted` PSPs which we can apply as on the existing cluster.

#### PSP Deprecation

PSPs are deprecated as of k8s 1.21 and will be removed 1.25, so in a year. This is due to them being difficult to specify. It's not clear what will replace them, but OPA Gatekeeper is a possibility. We will look to move from PSPs to whatever standard becomes dominant over coming months.

### Apps' access to AWS roles

**Chosen:** IAM Roles for Service Accounts (IRSA), instead of kiam

**Chosen:** Block access to instance metadata's node role using iptables

#### IRSA

Benefits of IRSA over kiam:

* IRSA is slightly improved security, compared to kiam. IRSA injects the token into the pod, to provide the credentials. kiam instead works by configuring iptables to intercept access to the instance metadata, and the concern is that there may be time betwen node boot and this configuration.

#### Blocking access instance metadata

The EKS node needs various AWS permissions, and these are provided by the node role. To avoid pods using these persmissions, we should block access. Implementation details are provided here: https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node and we will use kube2iam to achieve this.
