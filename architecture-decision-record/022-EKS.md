# EKS

Date: 02/05/2021

## Status

✅ Accepted

## What’s proposed

Use Amazon EKS for running the main cluster, which hosts MOJ service teams' applications. This replaces usage of kOps.

## Reasoning

Benefits of EKS:

* a managed control plane, reducing operational overhead compared to kOps, such as scaling the control plane nodes
* managed nodes, further reducing operational overhead

We've gained a lot experience of using EKS with the Manager cluster

## EKS configuration decisions

### Auth

**Chosen:**: OIDC, with Auth0 as broker and GitHub as Identity Provider

Developers in service teams need to use the k8s auth, and GitHub continues to be the most common SSO amongst them with good tie-in to JML processes - see [ADR 6 Use GitHub as our identity provider](006-Use-github-as-user-directory.md)

Auth0 is useful as a broker, because it adds the GitHub team, used by k8s RBAC authorization. (check)

Future options:

* Azure AD SSO is growing in MOJ - there's a case for switching to that, if it is adopted amongst our users
* IAM auth has the benefit of immediately revoking access. Maybe we could use federated login with GitHub? (But would that give only temporary kubecfg?) Or sync the GitHub team info to IAM?

### Auth credential issuer

**Chosen:**: Cloud Platform Kuberos

We've long used Kuberos for issuing kubecfg credentials to users. The [original version of Kuberos](https://github.com/negz/kuberos) is unmaintained, but it's pretty simple, so we are updating the dependencies in: https://github.com/ministryofjustice/cloud-platform-kuberos/

Other options considered:

* [Gangway](https://github.com/heptiolabs/gangway) - similar to Kuberos, it has not had releases for 2 years (v3.2.0)
* [kubelogin](https://github.com/int128/kubelogin)
  * CP team would have to distribute the client secret to all users. It seems odd to go to the trouble of securely sharing that secret, to overcome the perceived difficulty of issuing kubecfg credentials.
  * Requires all of our users to install the software, rather than doing it server-side centrally
* [kubehook](https://github.com/negz/kubehook) - not compatible with EKS - doesn't support web hook authn
* [dex](https://github.com/dexidp/dex) - doesn't have a web front-end for issuing creds - it is more of an OIDC broker

### Authorization

**Chosen:**: existing RBAC configuration

We'll continue to use our existing RBAC configuration from the previous cluster.

### Node management

**Chosen:**: Managed node groups, with future experimenting with Fargate

Options:

* Self-managed nodes
* Managed node groups - automates various aspects of the node lifecycle, including creating the EC2s, the auto scaling group, registration of nodes with kubernetes and recycling nodes
* Fargate nodes - fully automated nodes, the least to manage. Benefits from more isolation between pods and automatic scaling. Doesn't support daemonsets.

We aim to take advantage of as much automation as possible, to minimize the team's operational overhead and risk. Initially we'll use managed node groups, before looking at Fargate for workloads.

#### Future Fargate considerations

*Pod limits* - there is a quota limit of [500 Fargate pods per region per AWS Account](https://aws.amazon.com/about-aws/whats-new/2020/09/aws-fargate-increases-default-resource-count-service-quotas/) which could be an issue, considering we currently run ~2000 pods. We can request AWS raise the limit - not currently sure what scope there is. With Multi-cluster stage 5, the separation of loads into different AWS accounts will settle this issue.

*Daemonset functionality* - needs replacement:

* fluent-bit - currently used for log shipping to ElasticSearch. AWS provides a managed version of [Fluent Bit on Fargate](https://aws.amazon.com/blogs/containers/fluent-bit-for-amazon-eks-on-aws-fargate-is-here/) which can be configured to ship logs to ElasticSearch.
* prometheus-node-exporter - currently used to export node metrics to prometheus. In Fargate the node itself is managed by AWS and therefore hidden. However we can [collect some useful metrics about pods running in Fargate from scraping cAdvisor](https://aws.amazon.com/blogs/containers/monitoring-amazon-eks-on-aws-fargate-using-prometheus-and-grafana/), including on CPU, memory, disk and network

*No EBS support* - Prometheus will run still in a managed node group. Likely other workloads too to consider.

*how people check the status of their deployments* - to be investigated

*ingress can't be nginx? - just the load balancer in front* - to be investigated

If we don't use Fargate then we should take advantage of Spot instances for reduced costs. However Fargate is the priority, because the main driver here is engineer time, not EC2 cost.

### Node instance types

**Chosen:** r5.xlarge, r4.xlarge, c5.xlarge

Existing cluster uses:

* r5.xlarge - memory optimized range - 4 cores 32 GB
* r4.xlarge - memory optimized range - 4 cores 30.5 GB
* c5.xlarge - compute optimized range - 4 cores 8 GB

The emphasis on memory optimized nodes is born of experience of running out of memory more than CPU. These nodes work for the current workload, so we will continue with them for the time being.

### Node image

**Chosen:** Amazon EKS optimized AMIs

Link: https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html

These EKS AMIs are the default and therefore lowest maintenance. We can't see a reason not to use them. (If we needed changes, then a launch config would give us flexibility.)

### Node groups

**Chosen:** One node group initially, with a couple of instance types

Start with one node group, to minimize the operation overhead.

In the node group we will specify 2 or 3 related instance types, because with only 1 there is a risk that the cloud provider runs out of that type within the region.

Further node groups may be valueable for using Spot or Fargate nodes, as we explore them in the future.

### Pod networking (CNI)

**Chosen:** AWS VPC networking (CNI)

Link: https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html

AWS's CNI is used for the pod networking (IPAM, CNI and Routing). Each pod is given an ENI and IP address from the underlying VPC (rather than an overlay network). Pod traffic is routed between nodes using the native VPC.

Advantages of AWS's CNI:

* it is the default with EKS, native to AWS, is fully supported by AWS - low management overhead
* offers good network performance

The concern with this solution would be that it uses an ENI / IP address for every pod, and there is a [limit per node](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt), depending on the EC2 instance type. Our current `r5.xlarge` is limited to 58 pods per node. Currently on kops we have `kubectl get nodes | grep node | wc -l` 29 nodes, and the EKS equivalent would be `kubectl get pods -A | wc -l` 1926 pods / 58 = 33 nodes. These extra 4 nodes would cost $0.296*24*30 = $213/month, which is manageable. Other instances with higher limits are also possible (up to the kubelet limit of 110 pods per node). There is also a limit of IP addresses per subnet, and our /13 subnets have 8192 IPs - this should be plenty for 1926 pods plus lots of growth.

The alternative considered was [Calico networking](https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-calico-networking). This has the advantage of not needing an IP address per pod, and associated instance limit. And it is open source. However:

* Installation requires recycling the nodes, which is not a good fit with declarative config.
* We wouldn't have any support from the cloud provider if there were networking issues.
* It's likely that changes to EKS over time will frequently cause breakages with this networking setup.

### Network policy enforcement

**Chosen:** Calico network policy enforcement (i.e. just for policy, not the full blown Calico networking)

Link: https://docs.projectcalico.org/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-amazon-vpc-networking

This implements the Kubernetes Network Policy API, needed to isolate tenants. It is [recommended by EKS](https://docs.aws.amazon.com/eks/latest/userguide/calico.html). The CP team used it in the previous live-1 cluster, and are familiar with it.

CP's namespaces have a [default NetworkPolicy](https://github.com/ministryofjustice/cloud-platform-environments/blob/main/namespace-resources-cli-template/04-networkpolicy.yaml) that only allows incoming requests from pods in the same namespace and ingress-controller namespace, not other tenants' namespaces.

An alternative is [AWS Security Groups](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html). Creating a 'SecurityGroupPolicy' resource for a namespace would attach a security group, allowing us to specify the inbound and outbound network traffic. The [advantage](https://github.com/aws/containers-roadmap/issues/177) over a similar NetworkPolicy, is that the Security Group can specify AWS resources like RDS instances. So it would be a network restriction, in addition to the auth restriction (CP's RDS instances are currently [network accessible across the VPC](https://user-guide.cloud-platform.service.justice.gov.uk/documentation/other-topics/rds-external-access.html#accessing-your-rds-database)). We should explore this in the future for defence in depth.

**Chosen:** Calico Typha

Install Typha for performance benefits. It caches requests made by Calico Felix to the k8s API.

### Control plane logging

**Chosen:** enable all cluster logging

Link: https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html

Control plane logs can be enabled to log: k8s API, authentication, controller manager and scheduler

The logs go to CloudWatch. Maybe we need to export them elsewhere. Further discussion is covered in: [ADR 23 Logging](023-Logging.md)

### Node terminal access

**Chosen:** AWS Systems Manager, installed as daemonset; no bastion nodes

Links:

* https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/install-ssm-agent-on-amazon-eks-worker-nodes-by-using-kubernetes-daemonset.html
* https://github.com/aws/containers-roadmap/issues/593

AWS Systems Manager benefits:

* easy to install - daemonset
* auth is via a team member's AWS creds, so it's tied into JML processes and access can be removed immediately if they leave the team
* terminal commands are logged - useful for audit purposes
* [it's an EKS best practice](https://aws.github.io/aws-eks-best-practices/security/docs/hosts/#minimize-access-to-worker-nodes)

To note:

* requires permissions `hostNetwork: true` and `privileged: true` so may need its own PSP
* it's no use if the node is failing to boot or join the cluster properly, but we can live with that - it's likely that it's the pods we want to characterize, not the node, because the node is managed

The traditional method of node access would be to SSH in via a bastion. This involves a shared ssh key, and shared credentials is not an acceptable security practice.

### PodSecurityPolicies

**Chosen:** Default EKS PSP `eks.privileged` should be used only by `aws-node` pod

**Chosen:** Apply CP's existing `privileged` PSP only to the `kube-system` namespace and `restricted` PSP to the rest

EKS comes with PSP `eks.privileged` that allows anything. However, security best practice is to limit it, to ensure most pods aren't allowed to run as privileged or root. [EKS best practices says]: "we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace". In practice we found only `aws-node` pod needed it.

CP already has `privileged` and `restricted` PSPs which we can apply as on the existing cluster.

#### PSP Deprecation

PSPs are [deprecated](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/) as of k8s 1.21 (April 2021) and will be removed 1.25 (due April 2022). This is because PSPs are considered a difficult way to specify policy. It's not clear what will replace them, but OPA Gatekeeper is a possibility. We will look to move from PSPs to whatever standard is selected in the KEP over coming months.

### Apps' access to AWS roles

**Chosen:** IAM Roles for Service Accounts (IRSA), instead of kiam

**Chosen:** Block access to instance metadata's node role using iptables

#### IRSA

Benefits of IRSA over kiam:

* IRSA is slightly improved security, compared to kiam. IRSA injects the token into the pod, to provide the credentials. kiam instead works by configuring iptables to intercept access to the instance metadata, and the concern is that there may be time betwen node boot and this configuration.

#### Blocking access instance metadata

The EKS node needs various AWS permissions, and these are provided by the node role. To avoid pods using these persmissions, we should block access. Implementation details are provided here: https://aws.github.io/aws-eks-best-practices/security/docs/iam/#restrict-access-to-the-instance-profile-assigned-to-the-worker-node and we will use kube2iam to achieve this.
